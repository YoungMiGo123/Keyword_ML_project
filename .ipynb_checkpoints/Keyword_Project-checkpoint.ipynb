{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"sicmap.txt\", \"r\")\n",
    "obj = []\n",
    "resObj = {\"sic\": \"\", \"description\" : \"\", \"link\": \"\"}\n",
    "for x in f:\n",
    "    list = x.split('\\t')\n",
    "    \n",
    "    if len(list) == 3:\n",
    "        obj.append((list[0],list[1], list[2].replace(\"\\n\",\"\")))\n",
    "    elif len(list) == 2:\n",
    "        obj.append((list[0],list[1],\"\"))\n",
    "\n",
    "#print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "url = 'http://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1'\n",
    "url_2 = 'https://www.osha.gov/pls/imis/sic_manual.display?id=1002&tab=description'\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def GetKeywords(path):\n",
    "    response = get(path)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    elements = html_soup.find('span', {'class': 'blueTen'}).next_element.next_element\n",
    "    list = elements.get_text().split(', ')\n",
    "    return list\n",
    "\n",
    "def PrintData(code,title,link,data_list):\n",
    "    print(f\"Sic Code={code}, title={title}, link={link}\\nkeywords={data_list}\\n\\n\")\n",
    "#print(list)\n",
    "\n",
    "train = []\n",
    "count = 0\n",
    "\n",
    "for i,j,k in obj:\n",
    "    if 'description' in k:\n",
    "        keywords = GetKeywords(k)\n",
    "        train.append((i,j,k,keywords))\n",
    "        PrintData(i,j,k,keywords)\n",
    "        count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[\n",
    "    (111,'Growing of cereals (except rice), leguminous crops and oil seeds'),\n",
    "    (112,'Growing of rice'),\n",
    "    (113,'Growing of vegetables and melons, roots and tubers'),\n",
    "    (114,'Growing of sugar cane'),\n",
    "    (115,'Growing of tobacco'),\n",
    "    (116,'Growing of fibre crops'),\n",
    "    (119,'Growing of other non-perennial crops'),\n",
    "    (121,'Growing of grapes'),\n",
    "    (122,'Growing of tropical and subtropical fruits'),\n",
    "    (123,'Growing of citrus fruits'),\n",
    "    (124,'Growing of pome fruits and stone fruits'),\n",
    "    (125,'Growing of other tree and bush fruits and nuts'),\n",
    "    (126,'Growing of oleaginous fruits'),\n",
    "    (127,'Growing of beverage crops'),\n",
    "    (128,'Growing of spices, aromatic, drug and pharmaceutical crops'),\n",
    "    (129,'Growing of other perennial crops'),\n",
    "    (130,'Plant propagation')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKeywords(path):\n",
    "    response = get(path)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    elements = html_soup.find_all('span', {'class': 'item'})\n",
    "    list =[]\n",
    "    for i in elements:\n",
    "        list.append(i.get_text())\n",
    "    return list\n",
    "\n",
    "def PrintDict(dictionary):\n",
    "    for key in dictionary:\n",
    "        print(f'Sic key = {key}, value = {dictionary[key]}\\n')\n",
    "\n",
    "final_path = 'https://www.siccode.co.uk/sic2007/code-'\n",
    "\n",
    "#x = GetKeywords(final_path)\n",
    "#print(x)\n",
    "\n",
    "dict = {}\n",
    "for i, j in list:\n",
    "    strX = f'0{i}0'\n",
    "    correct_path = final_path + strX\n",
    "    dict[i] = GetKeywords(correct_path)\n",
    "\n",
    "PrintDict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv('Sic-Codes.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic_codes_list = df['SIC_Code']\n",
    "for i in sic_codes_list:\n",
    "    print(f'{i}\\n')\n",
    "sic_codes_description = df['Description']\n",
    "#print(sic_codes_list)\n",
    "#print(sic_codes_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path2 = 'https://www.siccode.co.uk/section/'\n",
    "list = [\n",
    "    ('a',1),\n",
    "    ('b',2),\n",
    "    ('c',3),\n",
    "    ('d',4),\n",
    "    ('e',5),\n",
    "    ('f',6),\n",
    "    ('g',7),\n",
    "    ('h',8),\n",
    "    ('i',9),\n",
    "    ('j',10),\n",
    "    ('k',11),\n",
    "    ('l',12),\n",
    "    ('m',13),\n",
    "    ('n',14),\n",
    "    ('o',15),\n",
    "    ('p',16),\n",
    "    ('q',17),\n",
    "    ('r',18),\n",
    "    ('s',19),\n",
    "    ('t',20),\n",
    "    ('u',21),\n",
    "]\n",
    "lowestLevelKeys = []\n",
    "\n",
    "def binary_search(listData, item):\n",
    "    for i in listData:\n",
    "        if i == item:  \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ContainsinList(item_list,item):\n",
    "    first = 0\n",
    "    last = len(item_list)-1\n",
    "    found = False\n",
    "    while( first<=last and not found):\n",
    "        mid = (first + last)//2\n",
    "        if item_list[mid] == item :\n",
    "            found = True\n",
    "        else:\n",
    "            if item < item_list[mid]:\n",
    "                last = mid - 1\n",
    "            else:\n",
    "                first = mid + 1\n",
    "    return found\n",
    "\n",
    "final_path = 'https://www.siccode.co.uk/sic2007/code-'\n",
    "final_count = 0\n",
    "test = \", \"\n",
    "import xlwt\n",
    "indx=0\n",
    "book= xlwt.Workbook()\n",
    "sheet1= book.add_sheet(\"write_to_xl1\")\n",
    "#for hit in lowestLevelKeys:\n",
    "#    i+=1\n",
    "\n",
    "\n",
    "\n",
    "for i, j in list:\n",
    "    \n",
    "    xpath = path2 + i\n",
    "    #gets lowest level class\n",
    "    var = GetKeywords(xpath)\n",
    "    \n",
    "    for code in var:\n",
    "        \n",
    "        x = code.split(':')\n",
    "        subclass = x[0].replace(\"Code \",\"\")\n",
    "        desc = x[1].strip()\n",
    "        superclass = 0\n",
    "        group = int(subclass[:3])\n",
    "        \n",
    "        if subclass[0] == '0':\n",
    "            superclass = int(subclass[1:len(subclass)-1])\n",
    "            group = subclass[1:3]\n",
    "            \n",
    "        if not subclass[0] == '0':\n",
    "            superclass = int(subclass[:len(subclass)-1])\n",
    "            \n",
    "        if ContainsinList(sic_codes_list,superclass):\n",
    "            #gets the keywords in the lowest class\n",
    "            datalist = GetKeywords(final_path + subclass)\n",
    "            final_count += 1            \n",
    "            resultKeys = test.join(datalist)\n",
    "            final_list = [f'{final_count}',f'{group}',f'{superclass}',f'{subclass}',desc,resultKeys]\n",
    "            print(f\"Record-{final_count} = {final_list}\")\n",
    "            for col_num, data in enumerate(final_list):\n",
    "                sheet1.write(indx,col_num,data)\n",
    "                #worksheet.write(0, col_num, data)\n",
    "            \n",
    "            indx += 1\n",
    "            \n",
    "            lowestLevelKeys.append(final_list)\n",
    "            #print(f'Group = {group} Super class Code = {superclass} Sub-class Code = {subclass}, Description = {desc}, keywords = {datalist}\\n')\n",
    "        \n",
    "    \n",
    "book.save('sic_codes.xls')\n",
    "\n",
    "#print(var)\n",
    "\n",
    "#print(lowestLevelKeys)\n",
    "#PrintDict(lowestLevelKeys)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for i,j in lowestLevelKeys:\n",
    "    if i[0] == '0':\n",
    "        trim = int(i[1:len(i)-1])\n",
    "       #if trim in sic_codes_list:\n",
    "       #    print(\"Success Starting 0\")\n",
    "    if not i[0] == '0'\n",
    "        sec_trim = int(i[:len(i)-1])\n",
    "        print(sec_trim)\n",
    "        if sec_trim in sic_codes_list:\n",
    "            print(\"Success ending 0\")\n",
    "    count += 1\n",
    "    if count == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lowestLevelKeys)\n",
    "import csv\n",
    "\n",
    "with open('sic_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(['group_id','class_id','subclass_id','description','keywords'])\n",
    "    list =[]\n",
    "    for i in lowestLevelKeys:\n",
    "        for x in i:\n",
    "            z = f'{i}'\n",
    "            list.append(z.encode(\"utf-8\"))\n",
    "        writer.writerows(list)\n",
    "        print(list)\n",
    "        list = []\n",
    "print('done')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('innovators.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['id',\"group_id\", \"class_id\", \"sub_class_id\",\"description\",'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process original keywords and scraped keywords\n",
    "import pandas as pd\n",
    "data = pd.read_excel('keywords.xlsx')\n",
    "#data.head(n=100)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes duplicates\n",
    "def removeDuplicates(oldlist):\n",
    "    newlist = list(set(oldlist))\n",
    "    return newlist\n",
    "\n",
    "#spell check\n",
    "def spell(a):\n",
    "    b = TextBlob(a)\n",
    "    correct = b.correct()\n",
    "    return correct\n",
    "\n",
    "\n",
    "\n",
    "#Lemmatization -- reducing the word to its root form as found in a dictionary.\n",
    "#options--> ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "def lemon(a,theType=\"blank\"):\n",
    "    b = Word(a)\n",
    "    \n",
    "    if(theType == \"blank\"):\n",
    "        correct = b.lemmatize()\n",
    "        return correct\n",
    "    else:\n",
    "        correct = b.lemmatize(theType)\n",
    "        return correct\n",
    "    \n",
    "#single    \n",
    "def single(a):\n",
    "    b = Word(a)\n",
    "    correct = b.singularize()\n",
    "    return correct\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "\n",
    "resultData = data['Column1']\n",
    "\n",
    "\n",
    "# sample text for performing tokenization\n",
    "text =  \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"\n",
    "datalist = []\n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "\n",
    "\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "\n",
    "#for i in resultData:\n",
    "#    #print(i)\n",
    "#    if type(i) == str:\n",
    "#        tokens = word_tokenize(i) \n",
    "#        fdist = FreqDist(token)\n",
    "#        #print(tokens)\n",
    "#        fdist\n",
    "#    #datalist.append(tokens)\n",
    "\n",
    "#print(datalist)\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = ['giving', 'given', 'given', 'gave']\n",
    "for word in stm:\n",
    "    print(word+ ' : ' +lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stm = ['giving', 'given', 'given', 'gave']\n",
    "for word in stm:\n",
    "    print(f'{word} :', lemmatizer.lemmatize(word)) \n",
    "print('rocks :', lemmatizer.lemmatize('rocks')) \n",
    "print('corpora :', lemmatizer.lemmatize('corpora'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cristiano', 'NN')]\n",
      "[('ronaldo', 'NN')]\n",
      "[('born', 'NN')]\n",
      "[('february', 'JJ')]\n",
      "[('5', 'CD')]\n",
      "[(',', ',')]\n",
      "[('1985', 'CD')]\n",
      "[(',', ',')]\n",
      "[('funchal', 'NN')]\n",
      "[(',', ',')]\n",
      "[('madeira', 'NN')]\n",
      "[(',', ',')]\n",
      "[('portugal', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# importing stopwors from nltk library\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "text = 'Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.'\n",
    "text1 = word_tokenize(text.lower())\n",
    "#print(f'Token keyowrds {text1}')\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "for token in stopwords:\n",
    "    print(nltk.pos_tag([token]))\n",
    "\n",
    "#print(f'Cleaned keywords {stopwords}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = 'vote to choose a particular man or a group (party) to represent them in parliament'\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = 'Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event'\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
